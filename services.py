# services.py
import os
import json
import time
from google import genai
from google.genai import types
from openai import OpenAI
from typing import Generator, Optional, Dict, Any
from models import ContentBlueprint, CreativeBrief
from prompts import (
    GEMINI_BRIEF_PROMPT,
    DEEPSEEK_BRIEF_PROMPT,
    OUTLINE_GENERATION_PROMPT,
    SECTION_TEXT_PROMPT
)
from templates import TEMPLATE_COGNITIVE_FLIP
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
import torch
from diffusers import StableDiffusionXLPipeline
from PIL import Image as PILImage
from dotenv import load_dotenv
load_dotenv()

class LLMService:
    def __init__(self, provider: str, model_name: str, api_key: Optional[str] = None):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key
        self.client = None
        self.local_model = None
        self.local_tokenizer = None
        self.supports_images = False  # Track if provider supports image input
        self.sd_pipeline = None  # Stable Diffusion pipeline for image generation
        
        # Set PyTorch memory configuration for better memory management
        if not os.getenv("PYTORCH_CUDA_ALLOC_CONF"):
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
        
        # Initialize Stable Diffusion if model path is available
        self.sd_model_path = None
        self._init_stable_diffusion()
        
        if provider == "DeepSeek":
            # if not api_key or api_key.strip() == "":
            #     raise ValueError("DeepSeek éœ€è¦æä¾› API Key")
            self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
        elif provider == "Google Gemini":
            # æ–°ç‰ˆ SDK åˆå§‹åŒ–æ–¹å¼
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ keyï¼Œå¦åˆ™è‡ªåŠ¨æŸ¥æ‰¾ç¯å¢ƒå˜é‡ GEMINI_API_KEY
            key_to_use = api_key if api_key else os.getenv("GEMINI_API_KEY")
            if key_to_use:
                self.client = genai.Client(api_key=key_to_use)
            else:
                # å°è¯•æ— å‚åˆå§‹åŒ–ï¼ˆä¾èµ–ç¯å¢ƒå˜é‡ï¼‰
                self.client = genai.Client()
        elif provider == "Qwen-VL":
            try:
                # Load Qwen3-VL model using the official approach
                print(f"Loading Qwen3-VL model from: {self.model_name}")
                
                # Load processor (replaces tokenizer for VL models)
                self.local_tokenizer = AutoProcessor.from_pretrained(self.model_name)
                
                # Load model with Qwen3VL-specific class
                self.local_model = Qwen3VLForConditionalGeneration.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map="auto"
                )
                
                # Qwen3-VL supports image input
                self.supports_images = True
                
                print(f"âœ“ Qwen3-VL model loaded successfully")
                
            except Exception as e:
                raise Exception(f"åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {str(e)}")
    
    def _init_stable_diffusion(self):
        """Initialize Stable Diffusion XL pipeline for image generation
        
        Note: Pipeline is loaded on-demand to save memory when using large VL models.
        """
        try:
            sd_model_path = os.getenv("SD_MODEL")
            
            if not sd_model_path:
                print("âš ï¸  SD_MODEL environment variable not set. Image generation disabled.")
                return
            
            if not os.path.exists(sd_model_path):
                print(f"âš ï¸  SD model path does not exist: {sd_model_path}")
                return
            
            # Store path but don't load yet - will load on-demand
            self.sd_model_path = sd_model_path
            self.sd_pipeline = None
            print("âœ“ Stable Diffusion path configured (will load on-demand)")
            
        except Exception as e:
            print(f"âš ï¸  Failed to configure Stable Diffusion: {str(e)}")
            self.sd_model_path = None
            self.sd_pipeline = None
    
    def _load_sd_pipeline(self):
        """Load Stable Diffusion pipeline on-demand"""
        if self.sd_pipeline is not None:
            return  # Already loaded
        
        if not hasattr(self, 'sd_model_path') or not self.sd_model_path:
            return
        
        try:
            print("Loading Stable Diffusion XL pipeline...")
            
            # If Qwen model is loaded on GPU, move SD to CPU to avoid OOM
            use_cpu = False
            if self.local_model is not None and torch.cuda.is_available():
                # Check available GPU memory
                gpu_memory_free = torch.cuda.mem_get_info()[0] / 1024**3  # GB
                if gpu_memory_free < 8:  # Need at least 8GB free for SDXL
                    print(f"âš ï¸  Low GPU memory ({gpu_memory_free:.2f}GB free). Loading SD on CPU.")
                    use_cpu = True
            
            self.sd_pipeline = StableDiffusionXLPipeline.from_pretrained(
                self.sd_model_path,
                torch_dtype=torch.float16 if not use_cpu else torch.float32,
                use_safetensors=True,
                variant="fp16" if not use_cpu else None
            )
            
            if use_cpu or not torch.cuda.is_available():
                self.sd_pipeline = self.sd_pipeline.to("cpu")
                print("âœ“ Stable Diffusion XL loaded on CPU")
            else:
                self.sd_pipeline = self.sd_pipeline.to("cuda")
                print("âœ“ Stable Diffusion XL loaded on GPU")
            
            # Enable memory efficient attention
            self.sd_pipeline.enable_attention_slicing()
            
        except Exception as e:
            print(f"âš ï¸  Failed to load Stable Diffusion: {str(e)}")
            self.sd_pipeline = None

    def _translate_brief_fields(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Translate Chinese field names to English for CreativeBrief model
        
        This handles cases where Qwen-VL generates Chinese field names despite instructions.
        """
        
        # Comprehensive mapping of Chinese to English field names
        field_mapping = {
            # Top-level fields
            'å®šä½é¶å¿ƒ': 'targeting',
            'æ ¸å¿ƒæ´å¯Ÿ': 'insight',
            'ä»·å€¼è·¨è¶Š': 'transformation',
            'æ²Ÿé€šç­–ç•¥': 'strategy',
            
            # Transformation fields
            'å½“å‰çŠ¶æ€': 'current_state',
            'ç°çŠ¶': 'current_state',
            'æœŸæœ›çŠ¶æ€': 'desired_state',
            'æ„¿æ™¯': 'desired_state',
            
            # Strategy fields
            'é’©å­ç±»å‹': 'hook_type',
            'é’©å­': 'hook_type',
            'æ²Ÿé€šè¯­æ°”': 'tone',
            'è¯­æ°”': 'tone',
            'äººè®¾': 'tone',
            
            # Already English (pass through)
            'targeting': 'targeting',
            'insight': 'insight',
            'transformation': 'transformation',
            'current_state': 'current_state',
            'desired_state': 'desired_state',
            'strategy': 'strategy',
            'hook_type': 'hook_type',
            'tone': 'tone'
        }
        
        translated = {}
        
        for key, value in data.items():
            # Translate key if it's in mapping, otherwise keep original
            english_key = field_mapping.get(key, key)
            
            # If value is a dict, recursively translate
            if isinstance(value, dict):
                translated[english_key] = self._translate_brief_fields(value)
            else:
                translated[english_key] = value
        
        return translated

    def _generate_with_qwen(self, prompt: str, image_path: Optional[str] = None, max_tokens: int = 1024, temperature: float = 0.9) -> str:
        """Helper method to generate text with Qwen3-VL model (supports text-only or text+image)"""
        try:
            # Prepare content list
            content = []
            
            # Add image if provided
            if image_path:
                from PIL import Image
                # Load image
                image = Image.open(image_path)
                content.append({"type": "image", "image": image})
            
            # Add text prompt
            content.append({"type": "text", "text": prompt})
            
            # Format messages for Qwen3-VL
            messages = [
                {
                    "role": "user",
                    "content": content
                }
            ]
            
            # Apply chat template with tokenization
            # When image is provided, processor handles it properly
            inputs = self.local_tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )
            inputs = inputs.to(self.local_model.device)
            
            # Generate
            with torch.no_grad():
                generated_ids = self.local_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.95,
                )
            
            # Trim input tokens from output
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
            ]
            
            # Decode using processor
            output_text = self.local_tokenizer.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )
            
            return output_text[0].strip() if output_text else ""
            
        except Exception as e:
            raise Exception(f"Qwen-VL generation failed: {str(e)}")

    def generate_creative_brief(self, fragments: str, image_path: Optional[str] = None) -> Optional[CreativeBrief]:
        """ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆåˆ›æ„ç®€æŠ¥"""
        if not (self.client or (self.local_model and self.local_tokenizer)):
            raise Exception(f"{self.provider} å®¢æˆ·ç«¯æœªæ­£ç¡®åˆå§‹åŒ–")
            
        try:
            if self.provider == "Google Gemini":
                prompt = GEMINI_BRIEF_PROMPT.format(fragments=fragments)
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        response_schema=CreativeBrief
                    )
                )
                return CreativeBrief(**json.loads(response.text))

            elif self.provider == "DeepSeek":
                prompt = DEEPSEEK_BRIEF_PROMPT.format(fragments=fragments)
                schema_hint = CreativeBrief.model_json_schema()
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": f"{prompt}\nJSON Schema: {json.dumps(schema_hint)}"}
                    ],
                    response_format={'type': 'json_object'},
                    temperature=1.3
                )
                
                content = response.choices[0].message.content
                if not content:
                    raise Exception("DeepSeek è¿”å›ç©ºå†…å®¹")
                
                # æ¸…ç†å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:]
                if content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                content = content.strip()
                
                # è§£æå¹¶éªŒè¯æ•°æ®
                data = json.loads(content)
                return CreativeBrief(**data)
            
            elif self.provider == "Qwen-VL":
                # Use the GEMINI brief prompt as the base for local generation
                base_prompt = GEMINI_BRIEF_PROMPT.format(fragments=fragments)
                
                # Add explicit JSON schema instruction for Qwen-VL with bilingual clarity
                schema_instruction = """
===== ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘CRITICAL OUTPUT FORMAT =====
ä½ å¿…é¡»è¾“å‡ºJSONæ ¼å¼ï¼Œä½†æ˜¯å­—æ®µåï¼ˆkeysï¼‰å¿…é¡»ç”¨è‹±æ–‡ï¼Œå†…å®¹ï¼ˆvaluesï¼‰ç”¨ä¸­æ–‡ã€‚

Required JSON structure (å­—æ®µåç”¨è‹±æ–‡ï¼Œå†…å®¹ç”¨ä¸­æ–‡):
{
  "targeting": "ä½ çš„å®šä½é¶å¿ƒå†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡targetingï¼‰",
  "insight": "ä½ çš„æ ¸å¿ƒæ´å¯Ÿå†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡insightï¼‰",
  "transformation": {
    "current_state": "å½“å‰çŠ¶æ€å†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡current_stateï¼‰",
    "desired_state": "æœŸæœ›çŠ¶æ€å†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡desired_stateï¼‰"
  },
  "strategy": {
    "hook_type": "é’©å­ç±»å‹å†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡hook_typeï¼‰",
    "tone": "æ²Ÿé€šè¯­æ°”å†…å®¹ï¼ˆç”¨ä¸­æ–‡å†™å†…å®¹ï¼Œä½†å­—æ®µåå¿…é¡»æ˜¯è‹±æ–‡toneï¼‰"
  }
}

âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆä¸è¦è¿™æ ·ï¼‰:
{
  "å®šä½é¶å¿ƒ": "...",  // å­—æ®µåä¸èƒ½ç”¨ä¸­æ–‡ï¼
  "æ ¸å¿ƒæ´å¯Ÿ": "..."
}

âœ… æ­£ç¡®ç¤ºä¾‹:
{
  "targeting": "å› ä¸ºå†™ä¸å‡ºè®ºæ–‡è€Œç„¦è™‘å¤±çœ çš„åšå£«ç”Ÿ",  // å­—æ®µåç”¨è‹±æ–‡ï¼Œå†…å®¹ç”¨ä¸­æ–‡
  "insight": "é™ä½å¿ƒç†é¢„æœŸï¼Œå…ˆå†™åƒåœ¾åˆç¨¿",
  "transformation": {
    "current_state": "é™·å…¥å®Œç¾ä¸»ä¹‰é™·é˜±ï¼Œä¸€ä¸ªå­—éƒ½å†™ä¸å‡ºæ¥",
    "desired_state": "æ¥å—åˆç¨¿å¯ä»¥å¾ˆçƒ‚ï¼Œå¼€å§‹åŠ¨ç¬”"
  },
  "strategy": {
    "hook_type": "åç›´è§‰ï¼šå®Œç¾ä¸»ä¹‰æ˜¯æ‹–å»¶çš„ç½ªé­ç¥¸é¦–",
    "tone": "ç†æ€§å­¦éœ¸ï¼Œä¸€é’ˆè§è¡€"
  }
}

é‡è¦æé†’ï¼š
- å­—æ®µåç§°ï¼ˆkeysï¼‰: targeting, insight, transformation, current_state, desired_state, strategy, hook_type, tone
- ä¸è¦ä½¿ç”¨: "å®šä½é¶å¿ƒ", "æ ¸å¿ƒæ´å¯Ÿ", "ä»·å€¼è·¨è¶Š", "å½“å‰çŠ¶æ€", "æœŸæœ›çŠ¶æ€", "æ²Ÿé€šç­–ç•¥", "é’©å­ç±»å‹", "æ²Ÿé€šè¯­æ°”"
===== END OF FORMAT =====
"""
                
                prompt = base_prompt + "\n\n" + schema_instruction
                
                # Generate with the local model (pass image_path if provided)
                text = self._generate_with_qwen(prompt, image_path=image_path, max_tokens=1024, temperature=0.9)
                
                content = text.strip()
                # strip possible code fences
                if content.startswith('```json'):
                    content = content[7:]
                if content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                content = content.strip()
                
                data = json.loads(content)
                
                # Debug: Log original structure before translation
                print(f"[DEBUG] Qwen-VL raw output keys: {list(data.keys())}")
                if 'transformation' in data:
                    print(f"[DEBUG] transformation keys: {list(data['transformation'].keys())}")
                elif 'ä»·å€¼è·¨è¶Š' in data:
                    print(f"[DEBUG] Found Chinese 'ä»·å€¼è·¨è¶Š', keys: {list(data['ä»·å€¼è·¨è¶Š'].keys())}")
                if 'strategy' in data:
                    print(f"[DEBUG] strategy keys: {list(data['strategy'].keys())}")
                elif 'æ²Ÿé€šç­–ç•¥' in data:
                    print(f"[DEBUG] Found Chinese 'æ²Ÿé€šç­–ç•¥', keys: {list(data['æ²Ÿé€šç­–ç•¥'].keys())}")
                
                # Translate Chinese field names to English if model still used them
                data = self._translate_brief_fields(data)
                
                print(f"[DEBUG] After translation keys: {list(data.keys())}")
                
                return CreativeBrief(**data)
            else:
                raise Exception(f"æœªçŸ¥çš„æœåŠ¡å•†: {self.provider}")

        except Exception as e:
            raise Exception(f"åˆ›æ„ç®€æŠ¥ç”Ÿæˆå¤±è´¥: {str(e)}")

    def generate_blueprint(self, fragments: str, style: str, image_path: Optional[str] = None) -> Optional[ContentBlueprint]:
        """ç¬¬äºŒæ­¥ï¼šåŸºäºåˆ›æ„ç®€æŠ¥å’Œæ¨¡æ¿ç”Ÿæˆå¤§çº²"""
        
        # å…ˆç”Ÿæˆåˆ›æ„ç®€æŠ¥
        brief = self.generate_creative_brief(fragments, image_path=image_path)
        if not brief:
            return None
        
        # å‡†å¤‡æ•°æ®ï¼šå°†åˆ›æ„ç®€æŠ¥å’Œæ¨¡æ¿åºåˆ—åŒ–ä¸º JSON
        brief_json = json.dumps({
            "targeting": brief.targeting,
            "insight": brief.insight,
            "current_state": brief.transformation.current_state,
            "desired_state": brief.transformation.desired_state,
            "hook_type": brief.strategy.hook_type,
            "tone": brief.strategy.tone
        }, ensure_ascii=False, indent=2)
        
        template_json = json.dumps(TEMPLATE_COGNITIVE_FLIP, ensure_ascii=False, indent=2)
        
        # ä½¿ç”¨æ–°çš„å¤§çº²ç”Ÿæˆ prompt
        prompt = OUTLINE_GENERATION_PROMPT.format(
            brief_json=brief_json,
            template_json=template_json
        )

        try:
            if self.provider == "Google Gemini":
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json"
                    )
                )
                outline_data = json.loads(response.text)
                
                # è½¬æ¢æ¨¡æ¿è¾“å‡ºä¸º OutlineSection æ ¼å¼
                sections = []
                if 'outline' in outline_data:
                    for idx, section in enumerate(outline_data['outline'], 1):
                        # ä»æ¨¡æ¿æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®
                        template_section = next(
                            (s for s in TEMPLATE_COGNITIVE_FLIP['structure'] if s['section_id'] == section.get('section_id')),
                            None
                        )
                        
                        sections.append({
                            'id': idx,
                            'title': section.get('title', ''),
                            'intent': template_section['content_instruction'] if template_section else section.get('draft_content', ''),
                            'key_points': [section.get('draft_content', '')]  # å°†draft_contentä½œä¸ºå…³é”®ç‚¹
                        })
                
                # Pass dict data directly, let Pydantic instantiate OutlineSection
                return ContentBlueprint(brief=brief, outline=sections)

            elif self.provider == "DeepSeek":
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    response_format={'type': 'json_object'},
                    temperature=1.3
                )
                outline_data = json.loads(response.choices[0].message.content)
                
                # è½¬æ¢æ¨¡æ¿è¾“å‡ºä¸º OutlineSection æ ¼å¼
                sections = []
                if 'outline' in outline_data:
                    for idx, section in enumerate(outline_data['outline'], 1):
                        # ä»æ¨¡æ¿æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®
                        template_section = next(
                            (s for s in TEMPLATE_COGNITIVE_FLIP['structure'] if s['section_id'] == section.get('section_id')),
                            None
                        )
                        
                        sections.append({
                            'id': idx,
                            'title': section.get('title', ''),
                            'intent': template_section['content_instruction'] if template_section else section.get('draft_content', ''),
                            'key_points': [section.get('draft_content', '')]  # å°†draft_contentä½œä¸ºå…³é”®ç‚¹
                        })
                
                # Pass dict data directly, let Pydantic instantiate OutlineSection
                return ContentBlueprint(brief=brief, outline=sections)

            elif self.provider == "Qwen-VL":
                # Add explicit JSON schema instruction for Qwen-VL
                schema_instruction = """
You MUST respond with a JSON object using EXACTLY these English field names:
{
  "outline": [
    {
      "section_id": "string",
      "title": "string", 
      "draft_content": "string"
    }
  ]
}

IMPORTANT: Use ONLY English field names. Do not translate to Chinese.
"""
                
                prompt_with_schema = prompt + "\n\n" + schema_instruction
                
                # Local generation for outline JSON
                text = self._generate_with_qwen(prompt_with_schema, max_tokens=1500, temperature=0.9)
                
                content = text.strip()
                # try to clean fences
                if content.startswith('```json'):
                    content = content[7:]
                if content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                content = content.strip()
                
                outline_data = json.loads(content)

                # è½¬æ¢æ¨¡æ¿è¾“å‡ºä¸º OutlineSection æ ¼å¼
                sections = []
                if 'outline' in outline_data:
                    for idx, section in enumerate(outline_data['outline'], 1):
                        # ä»æ¨¡æ¿æŸ¥æ‰¾å¯¹åº”çš„å…ƒæ•°æ®
                        template_section = next(
                            (s for s in TEMPLATE_COGNITIVE_FLIP['structure'] if s['section_id'] == section.get('section_id')),
                            None
                        )

                        sections.append({
                            'id': idx,
                            'title': section.get('title', ''),
                            'intent': template_section['content_instruction'] if template_section else section.get('draft_content', ''),
                            'key_points': [section.get('draft_content', '')]
                        })

                # Pass dict data directly, let Pydantic instantiate OutlineSection
                return ContentBlueprint(brief=brief, outline=sections)

        except Exception as e:
            raise Exception(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")

    def generate_section_text(self, section: 'OutlineSection', brief: CreativeBrief, section_idx: int) -> str:
        """ç¬¬ä¸‰æ­¥ï¼šåŸºäºæ¨¡æ¿å’Œå¤§çº²ç”Ÿæˆå•ç« èŠ‚æ­£æ–‡"""
        
        # ä»æ¨¡æ¿ä¸­è·å–å¯¹åº”ç« èŠ‚çš„å…ƒæ•°æ®
        template_sections = TEMPLATE_COGNITIVE_FLIP['structure']
        template_section = template_sections[section_idx] if section_idx < len(template_sections) else None
        
        if not template_section:
            return "[é”™è¯¯: æ— æ³•æ‰¾åˆ°å¯¹åº”çš„æ¨¡æ¿ç« èŠ‚]"
        
        # å‡†å¤‡ prompt å‚æ•°
        prompt = SECTION_TEXT_PROMPT.format(
            section_role=template_section['role'],
            section_title=section.title,
            content_instruction=template_section['content_instruction'],
            word_count_limit=template_section.get('word_count_limit', 200),
            targeting=brief.targeting,
            insight=brief.insight,
            current_state=brief.transformation.current_state,
            desired_state=brief.transformation.desired_state,
            tone=brief.strategy.tone
        )

        try:
            if self.provider == "Google Gemini":
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        response_mime_type="text/plain"
                    )
                )
                return response.text if response.text else ""

            elif self.provider == "DeepSeek":
                # === ä¿®å¤ï¼šDeepSeek å…³é—­æµå¼ ===
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    stream=False  # æ˜¾å¼å…³é—­æµå¼
                )
                return response.choices[0].message.content

            elif self.provider == "Qwen-VL":
                # Calculate appropriate max tokens based on word count limit
                max_tokens = template_section.get('word_count_limit', 200) * 3
                
                text = self._generate_with_qwen(prompt, max_tokens=max_tokens, temperature=0.9)
                return text

        except Exception as e:
            return f"[ç”Ÿæˆå‡ºé”™: {str(e)}]"
    
    def generate_illustration(self, section: 'OutlineSection', brief: CreativeBrief, section_text: str) -> Optional[str]:
        """Generate an illustration image for a section using Stable Diffusion
        
        Args:
            section: The outline section
            brief: The creative brief for context
            section_text: The generated text content for this section
            
        Returns:
            Path to saved image file, or None if generation failed
        """
        # Load SD pipeline on-demand
        self._load_sd_pipeline()
        
        if not self.sd_pipeline:
            print("âš ï¸  Stable Diffusion not available")
            return None
        
        try:
            # Step 1: Use LLM to generate an image prompt based on the content
            image_prompt = self._generate_image_prompt(section, brief, section_text)
            
            if not image_prompt:
                return None
            
            print(f"ğŸ¨ Image prompt: {image_prompt}")
            
            # Step 2: Free up GPU memory if using Qwen-VL on GPU
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Step 3: Generate image with Stable Diffusion
            # Add quality enhancing keywords
            enhanced_prompt = f"{image_prompt}, high quality, detailed, professional illustration, clean design"
            negative_prompt = "text, watermark, signature, blurry, low quality, distorted, ugly, bad anatomy"
            
            image = self.sd_pipeline(
                prompt=enhanced_prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=30,
                guidance_scale=7.5,
                width=1024,
                height=1024
            ).images[0]
            
            
            # Step 5: Save image
            import tempfile
            output_dir = "outputs/images"
            os.makedirs(output_dir, exist_ok=True)
            
            image_filename = f"section_{section.id}_{int(time.time())}.png"
            image_path = os.path.join(output_dir, image_filename)
            image.save(image_path)
            
            print(f"âœ… Image saved to: {image_path}")
            return image_path
            
        except Exception as e:
            print(f"âš ï¸  Image generation failed: {str(e)}")
            return None
    
    def _generate_image_prompt(self, section: 'OutlineSection', brief: CreativeBrief, section_text: str) -> Optional[str]:
        """Use LLM to generate a Stable Diffusion prompt based on the content"""
        
        prompt_generation_instruction = f"""
Based on the following content, generate a concise Stable Diffusion prompt for an illustration image.

Section Title: {section.title}
Section Content: {section_text[:500]}...
Core Insight: {brief.insight}
Tone: {brief.strategy.tone}

Requirements:
1. Describe a single, clear visual concept (not multiple scenes)
2. Use concrete visual elements (colors, objects, atmosphere)
3. Match the tone: {brief.strategy.tone}
4. Keep it under 50 words
5. Focus on symbolic or metaphorical representation
6. NO text, NO people's faces (use silhouettes if needed)

Output ONLY the image prompt, nothing else. Use English for better SD results.

Example good prompts:
- "A minimalist geometric staircase ascending into clouds, soft gradient sky, hope and progress concept"
- "Tangled red threads slowly untangling into organized lines, overhead view, problem-solving metaphor"
- "A single bright lightbulb illuminating a dark cluttered desk, contrast between chaos and clarity"
"""
        
        try:
            if self.provider == "Google Gemini":
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt_generation_instruction,
                    config=types.GenerateContentConfig(
                        response_mime_type="text/plain"
                    )
                )
                return response.text.strip() if response.text else None
            
            elif self.provider == "DeepSeek":
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt_generation_instruction}],
                    stream=False,
                    temperature=0.8
                )
                return response.choices[0].message.content.strip()
            
            elif self.provider == "Qwen-VL":
                text = self._generate_with_qwen(prompt_generation_instruction, max_tokens=200, temperature=0.8)
                return text.strip()
            
            return None
            
        except Exception as e:
            print(f"âš ï¸  Image prompt generation failed: {str(e)}")
            return None